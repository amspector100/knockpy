{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll review how to use ``knockpy`` to apply the knockoff framework for variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick review of knockoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we briefly review the knockoff framework. Users already familiar with knockoffs may want to scroll past this section.\n",
    "\n",
    "Given a set of $p$ features $X = (X_1, \\dots, X_p)$ and an outcome of interest $y$, knockoffs aim to select the small fraction of features on which $y$ actually depends while controlling the false discovery rate. For example, if $y \\mid X \\sim \\mathcal{N}(X \\beta, \\sigma^2)$ and $\\beta$ is sparse, knockoffs aim to identify the set $\\{j : \\beta_j \\ne 0\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the knockoffs framework involves executing three steps.\n",
    "\n",
    "1. First, we construct synthetic variables $\\tilde{X} = (\\tilde{X}_1, \\dots, \\tilde{X}_p)$ called knockoffs. Intuitively, the $j$th knockoff $\\tilde{X}_j$ acts as a \"negative control\" on the $j$th feature $X_j$ during variable selection. In ``knockpy``, knockoffs are denoted as the numpy array ``Xk``. \n",
    "\n",
    "2. Second, we use an arbitrary machine learning algorithm -- usually called a *feature statistic* -- to assign variable importances to each of the $p$ features and each of the $p$ knockoffs. For example, we might train a cross-validated Lasso on $[X, \\tilde{X}]$ and $y$ and use the lasso coefficient sizes as a measure of variable importance.\n",
    "\n",
    "3. Intuitively, a non-null feature should be assigned a higher variable importance than its (synthetic) knockoff, whereas knockoffs are constructed such that null features are indistinguishable from their knockoff. The *data-dependent-threshhold* introduced in [Barber and Candes 2015](https://arxiv.org/abs/1404.5609) formalizes this intuition and uses the feature statistics to reject a set of variables such that the expected fraction of false positives is below a prespecified proportion $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of knockoffs:\n",
    "\n",
    "1. **Fixed-X** knockoffs treat the design matrix $X$ as fixed and control the false discovery rate assuming $y \\mid X$ follows a homoskedastic gaussian linear response. In this case, it is possible to construct valid knockoffs $\\tilde{X}$ with no assumptions on $X$. Note also that when using fixed-X knockoffs, the feature statistic must satisfy a slightly more restrictive *sufficiency* condition (see [Barber and Candes 2015](https://arxiv.org/abs/1404.5609)).\n",
    "\n",
    "2. **Model-X** knockoffs treat the design matrix $X$ as random. Model-X knockoffs control the false discovery rate for any conditional distribution $y \\mid X$, but they assume that the distribution of $X$ is known. Thus, to construct model-X knockoffs, one must know (or estimate) the distribution of $X$. See [Candes et al. (2018)](https://arxiv.org/abs/1610.02351) for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``KnockoffFilter`` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``knockpy.knockoff_filter.KnockoffFilter`` is the most important class in ``knockpy``: it generates knockoffs, fits the feature statistics, and applies the data-dependent threshhold all at once. This is demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a synthetic dataset where $X \\sim \\mathcal{N}(0, \\Sigma)$ for some $\\Sigma$ and $y \\mid X$ Gaussian with homoskedastic errors. The details of this dataset are commented below, but they aren't too important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knockpy version is: 1.0.3\n"
     ]
    }
   ],
   "source": [
    "# Create a random covariance matrix for X\n",
    "import numpy as np\n",
    "import knockpy\n",
    "import warnings\n",
    "print(f\"Knockpy version is: {knockpy.__version__}\")\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 300 # number of data points\n",
    "p = 500  # number of features\n",
    "Sigma = knockpy.dgp.AR1(p=p, rho=0.5) # Stationary AR1 process with correlation 0.5\n",
    "\n",
    "# Sample X\n",
    "X = np.random.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=(n,))\n",
    "\n",
    "# Create random sparse coefficients\n",
    "beta = knockpy.dgp.create_sparse_coefficients(p=p, sparsity=0.1)\n",
    "y = np.dot(X, beta) + np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ``KnockoffFilter`` class. To do this, we need to specify (i) what type of knockoff sampler we will use and (ii) what feature statistic we are using. Since $X \\sim \\mathcal{N}(0, \\Sigma)$, we will use Gaussian knockoffs, and we'll use the Lasso as our feature statistic, since it's a good all-around choice. We'll explore more options for these arguments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knockpy.knockoff_filter import KnockoffFilter\n",
    "kfilter = KnockoffFilter(\n",
    "    ksampler='gaussian',\n",
    "    fstat='lasso',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the knockoff filter on our data using the ``forward`` method. Since we are using a model-X approach, we initially pass $\\Sigma$ as an input to the knockoff filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The knockoff filter has discovered 100.0% of the non-nulls with a FDP of 13.793103396892548%\n"
     ]
    }
   ],
   "source": [
    "# Flags of whether each feature was rejected\n",
    "rejections = kfilter.forward(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    Sigma=Sigma,\n",
    "    fdr=0.1 # desired level of false discovery rate control\n",
    ")\n",
    "# Check the number of discoveries we made\n",
    "power = np.dot(rejections, beta != 0) / (beta != 0).sum()\n",
    "fdp = np.dot(rejections, beta == 0) / rejections.sum()\n",
    "print(f\"The knockoff filter has discovered {100*power}% of the non-nulls with a FDP of {100*fdp}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in most real applications, we do not know $\\Sigma$. In these cases, the knockoff filter will automatically infer $\\Sigma$ using LedoitWolf or GraphicalLasso covariance estimation. Although this invalidates the exact validty of model-X knockoffs, knockoffs have been shown to be fairly robust in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The knockoff filter has discovered 100.0% of the non-nulls with a FDP of 5.6603774428367615%\n"
     ]
    }
   ],
   "source": [
    "# Try again with estimated cov matrix\n",
    "kfilter2 = KnockoffFilter(ksampler='gaussian', fstat='lasso')\n",
    "rejections = kfilter.forward(X=X, y=y, fdr=0.1)\n",
    "# Check the number of discoveries we made\n",
    "power = np.dot(rejections, beta != 0) / (beta != 0).sum()\n",
    "fdp = np.dot(rejections, beta == 0) / rejections.sum()\n",
    "print(f\"The knockoff filter has discovered {100*power}% of the non-nulls with a FDP of {100*fdp}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian knockoffs galore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to generate Gaussian knockoffs. The default option is to generate MVR knockoffs, which (informally) maximize $\\text{Var}(X_j \\mid X_{-j}, \\tilde{X})$ for each feature $X_j$, where $X_{-j}$ denotes all of the features except the $j$th feature. Intuitively, this  *minimizes the varianced-based reconstructability (MVR)* between the features and their knockoffs, preventing a feature statistic like a lasso or a randomforest from using the other features and knockoffs to *reconstruct* non-null features. See [Spector and Janson (2020)](https://arxiv.org/abs/2011.14625) for more details. \n",
    "\n",
    "There are a variety of other options to choose from, including:\n",
    "\n",
    "- MAXENT knockoffs maximize the entropy of $[X, \\tilde{X}]$, which is equivalent to minimizing the mutual information between $X$ and $\\tilde{X}$. See [Gimenez and Zou (2019)](https://arxiv.org/abs/1810.11378) or [Spector and Janson (2020)](https://arxiv.org/abs/2011.14625) for more details.\n",
    "- SDP knockoffs minimize the mean absolute covariance (MAC) between features and their knockoffs. These are often referred to as *SDP knockoffs*. These are currently the default choice for group knockoffs, although that might change!\n",
    "- Equicorrelated knockoffs also minimize the MAC, but with a constraint that increases computational efficiency and (usually) reduces statistical power. See [Barber and Candes 2015](https://arxiv.org/abs/1404.5609) for a discussion.\n",
    "- Conditional Independence (CI) knockoffs guarantee that each feature $X_j$ and its knockoff $\\tilde{X}_j$ are conditionally independent given all of the other features $X_{-j}$; however, CI knockoffs do not always exist, so we use a heuristic defined in [Ke et al. (2020)](https://arxiv.org/abs/2010.08132) in general.\n",
    "\n",
    "``knockpy`` supports all of these knockoff generation methods for \"gaussian\" and \"fx\" knockoff types. Naturally, it is also possible to use any of these types of knockoffs as proposals for the Metropolized knockoff sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses gaussian maxent knockoffs\n",
    "kfilter1 = KnockoffFilter(ksampler='gaussian', knockoff_kwargs={'method':'maxent'})\n",
    "\n",
    "# This uses fixed-X SDP knockoffs\n",
    "kfilter2 = KnockoffFilter(ksampler='fx', knockoff_kwargs={'method':'sdp'})\n",
    "\n",
    "# Metropolized sampler for heavy-tailed t markov chain using MVR-guided proposals\n",
    "kfilter3 = KnockoffFilter(ksampler='artk', knockoff_kwargs={'method':'mvr'})\n",
    "\n",
    "# The 'method' options include: equicorrelated, sdp, mvr, maxent, and ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knockpy provides this functionality by offering very fast solvers for generating the knockoff $S$-matrix, as detailed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For p=500, MVR took 1.8 sec, Maxent took 3.6 sec, SDP took 4.3 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Solve MVR, maxent, and SDP optimization problems for p = 500\n",
    "time0 = time.time()\n",
    "S_MVR = knockpy.smatrix.compute_smatrix(Sigma, method='mvr')\n",
    "time1 = time.time()\n",
    "mvr_time = np.around(time1 - time0, 1)\n",
    "S_MAXENT = knockpy.smatrix.compute_smatrix(Sigma, method='maxent')\n",
    "time2 = time.time()\n",
    "maxent_time = np.around(time2 - time1, 1)\n",
    "S_SDP = knockpy.smatrix.compute_smatrix(Sigma, method='sdp')\n",
    "time3 = time.time()\n",
    "sdp_time = np.around(time3 - time2, 1)\n",
    "print(f\"For p={Sigma.shape[0]}, MVR took {mvr_time} sec, Maxent took {maxent_time} sec, SDP took {sdp_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolized knockoff sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``knockpy`` implements a fully general covariance-guided Metropolized knockoff sampler, which is capable of sampling model-X knockoffs for any $X$ distribution given an (unnormalized) density function of $X$. This Metropolized knockoff sampler uses a variety of computational tricks to make it orders of magnitude faster than a naive implementation, although these tricks only work when the distribution of $X$ has conditional independence properties, as specified by an undirected graphical model. See [Bates et al. (2020)](https://arxiv.org/abs/1903.00434) for more details.\n",
    "\n",
    "The API reference as well as the source for the metro module details more advanced usage of metro; however, for now, we simply demonstrate how to pass in an arbitrary log-likelihood function to the ``MetropolizedKnockoffSampler`` class and use it to sample knockoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import knockpy.metro\n",
    "\n",
    "# Fake variables for simplicity.\n",
    "p = 30\n",
    "n = 300\n",
    "X_metro = np.random.randn(n, p)\n",
    "beta = knockpy.dgp.create_sparse_coefficients(p)\n",
    "y_metro = np.dot(X_metro, beta) + np.random.randn(n)\n",
    "\n",
    "# An arbitrary (unnormalized) log-likelihood function\n",
    "rhos = np.random.randn(p)\n",
    "def log_likelihood(X):\n",
    "    return np.sum(X[:, 0:-1]*rhos[0:-1]*np.abs(X[:, 1:]))\n",
    "    \n",
    "# Undirected graph \n",
    "U = np.zeros((p,p))\n",
    "for xcoord in range(p):\n",
    "    for offset in [-2, 1, 0, 1, 2]:\n",
    "        ycoord = min(max(0, xcoord + offset), p-1)\n",
    "        U[xcoord, ycoord] = 1\n",
    "        \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "metrosampler = knockpy.metro.MetropolizedKnockoffSampler(\n",
    "    log_likelihood, X=X_metro, undir_graph=U\n",
    ")\n",
    "Xk = metrosampler.sample_knockoffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to generate Metropolized knockoffs for discrete data via the ``buckets`` argument, which specifies the support of the discrete data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to build a custom knockoff sampler class, as long as it inherets from the base class ``knockpy.knockoffs.KnockoffSampler``, you can still pass it to the KnockoffFilter constructor to run the knockoff filter. For example, we pass the customized metropolized knockoff sampler to a KnockoffFilter below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfilter_metro = KnockoffFilter(ksampler=metrosampler, fstat='ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly (and redundantly) pass the knockoffs into the ``.forward`` call to achieve the same effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = kfilter_metro.forward(X=X_metro, y=y_metro, Xk=Xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metro module can also accept clique potentials from an undirected graphical model in place of the likelihood function to get a $O(p)$ speedup, as discussed in the API reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``knockpy`` offers a suite of built-in feature statistics, including cross-validated lasso, ridge, and group-lasso coefficients, lasso-path statistics, the deepPINK statistic [(Lu et. al 2018)](https://arxiv.org/abs/1809.01185), and random forest statistics with swap and swap integral importances [(Giminez et. al 2018)](https://arxiv.org/abs/1807.06214). One can easily call these use these feature statistics by modifying the ``fstat`` argument and ``fstat_kwarg`` arguments in the KnockoffFilter class, as exemplified below. See the API reference for more flags and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest statistics with swap importances\n",
    "kfilter1 = KnockoffFilter(ksampler='gaussian', fstat='randomforest')\n",
    "# Random forest with swap integral importances\n",
    "kfilter2 = KnockoffFilter(\n",
    "    ksampler='gaussian', \n",
    "    fstat='randomforest',\n",
    "    fstat_kwargs={'feature_importance':'swapint'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom feature statistics with ``FeatureStatistic``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``knockpy.knockoff_stats.FeatureStatistic`` class also has the ability to turn any model class with a ``fit`` or ``train`` method and a ``predict`` method into a knockoff feature statistic using the swap or swap integral importances introduced in swap and swap integral importances [(Giminez et. al 2018)](https://arxiv.org/abs/1807.06214). This means that after training a predictive model on $[X, \\tilde{X}]$ and $y$, to obtain a variable importance for variable $X_j$, the FeatureStatistic class temporarily replaces $X_j$ with $\\tilde{X}_j$ and records the decrease in predictive performance of the trained model. After repeating this for all features and knockoffs, this yields knockoff variable importances which can be transformed via an antisymmetric function to create valid feature statistics. \n",
    "\n",
    "Using this is easier than it sounds! To do so, begin by initializing any arbitrary model with train/fit and predict methods. Second, wrap it with a FeatureStatistic class. Lastly, pass the FeatureStatistic class to the KnockoffFilter, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made 35.0 rejections!\n"
     ]
    }
   ],
   "source": [
    "# Here, we use an arbitrary predictive model\n",
    "# (in this case, kernel ridge regression)\n",
    "# to create feature statistics\n",
    "\n",
    "# Step 1: Initialize the kernel ridge regression object\n",
    "import sklearn.kernel_ridge\n",
    "kridge = sklearn.kernel_ridge.KernelRidge(kernel='polynomial')\n",
    "\n",
    "# Step 2: Wrap it with a feature statistic\n",
    "kridge_fstat = knockpy.knockoff_stats.FeatureStatistic(model=kridge)\n",
    "\n",
    "# Step 3: Pass to a knockoff filter\n",
    "kfilter = KnockoffFilter(ksampler='gaussian', fstat=kridge_fstat)\n",
    "\n",
    "# Create synthetic dataset with nonlinear response\n",
    "dgprocess = knockpy.dgp.DGP()\n",
    "Xnonlin, ynonlin, _, _, _ = dgprocess.sample_data(\n",
    "    n=1000, p=200, cond_mean='cubic'\n",
    ")\n",
    "\n",
    "# Run the knockoff filter\n",
    "rejections = kfilter.forward(X=Xnonlin, y=ynonlin, fdr=0.2)\n",
    "print(f\"Made {rejections.sum()} rejections!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group knockoffs\n",
    "\n",
    "The KnockoffFilter offers some support for sampling group knockoffs for the gaussian and fixed-X knockoff types, as demonstrated below (see the API reference for more details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made 56.0 rejections!\n"
     ]
    }
   ],
   "source": [
    "p = X.shape[1]\n",
    "# Create arbitrary groupings of variables\n",
    "groups = np.arange(1, p+1, 1)\n",
    "groups = np.around(groups / 2 + 0.01).astype('int')\n",
    "# Initialize the filter as normal\n",
    "kfilter3 = KnockoffFilter(\n",
    "    ksampler='gaussian',\n",
    "    fstat='lasso',\n",
    ")\n",
    "# When running the filter, specify the groups \n",
    "rejections = kfilter3.forward(X, y, groups=groups, fdr=0.2)\n",
    "print(f\"Made {rejections.sum()} rejections!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that our current implementations for group knockoffs are in beta and are substantially slower than the ungrouped sampling methods, and the MVR/MAXENT methods use a heuristic gradient based method which lacks formal convergence guarantees. We plan to add more support for group knockoffs over the next few releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-dos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to release additional functionality for [conditional knockoffs](https://dmhuang.github.io/tutorials/cknockoff/index.html), more flavors of Metropolized knockoff samplers, and more --- stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
